{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8 - NLP\n",
    "\n",
    "\n",
    "## Sean McWhirter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import *\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in spreadsheet with links\n",
    "df = pd.read_excel(r'C:\\Users\\seans\\Desktop\\NLP\\HW5\\permalinks.xlsx', header=None)\n",
    "\n",
    "#convert to list to iterate through\n",
    "link_list = df[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.)\n",
    ">1.\tIn Python, load one of the sentiment vocabularies referenced in the textbook, and run the sentiment analyzer as explained in the corresponding reference. Add words to the sentiment vocabulary, if you think you need to, to better fit your particular text collection.\n",
    "\n",
    "I am using the TextBlob lexicon from the textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/16333296/how-do-you-create-nested-dict-in-python\n",
    "#https://docs.python.org/3/library/collections.html#collections.defaultdict\n",
    "\n",
    "#Using TextBlob\n",
    "reviews = []\n",
    "sentiment = []\n",
    "\n",
    "\n",
    "for i in link_list:\n",
    "    page = requests.get(i)\n",
    "    \n",
    "    #extract the html\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #Extract text we need from the tag\n",
    "    span = str(soup.find('div', attrs={'text show-more__control'}).text).strip().lower()\n",
    "    \n",
    "    #add it to the review list\n",
    "    reviews.append(span)\n",
    "    \n",
    "    #Blob it\n",
    "    blob = TextBlob(span)\n",
    "    \n",
    "    #Sentiment Analysis - this funciton removes punctuation and tokenizes \n",
    "    sent = blob.sentiment\n",
    "    \n",
    "    #add to sentiment list\n",
    "    sentiment.append(sent.polarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#make sure they are the same length--should be 100\n",
    "print(len(reviews))\n",
    "print(len(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the boys' is a fantastically written satire that was also the brainchild of the creative genius, simon pegg. karl urban was the perfect choice for butcher, with the vocabulary of negan (the walking dead) and the vigilante drive of paul kersey (death wish). there is so much material to work with that this series could continue for several seasons. kudos to seth rogen and amazon for bringing this brilliant concept to streaming media!\n",
      "0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "#It looks like the sentiment is correct for this random sample\n",
    "print(reviews[11])\n",
    "print(sentiment[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the three levels of sentiment\n",
    "x = 0.05\n",
    "pos_neg = []\n",
    "for i in sentiment:\n",
    "    if i> x:\n",
    "        pos_neg.append('Positive')\n",
    "    elif i < -x:\n",
    "         pos_neg.append('Negative')\n",
    "    else:\n",
    "         pos_neg.append('Neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack into matrix; add a column for the 'positive'/'negative'/'neutral' ratings\n",
    "sent_matrix = np.column_stack((reviews, sentiment,pos_neg))\n",
    "sent_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"started watching this brilliant spin on a superhero show after dinner. now it's 4 am and i have just watched the whole thing.from the very first episode i was glued to this exciting, interesting and crazy ride.\"\n",
      " '0.2708333333333333' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "print(sent_matrix[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of positive/negative/neutral reviews\n",
    "\n",
    "#https://stackoverflow.com/questions/60000941/counting-number-of-specific-numbers-in-the-column-of-a-numpy-array-in-python\n",
    "results, counts = np.unique(sent_matrix[:,2], return_counts=True)\n",
    "results_dict = dict(zip(results, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Resutls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Negative': 11, 'Neutral': 19, 'Positive': 70}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the results\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.)\n",
    ">2.\tFor each of the clusters you created in homework 7, compute the average, median, high, and low sentiment scores for each cluster. Explain whether you think this reveals anything interesting about the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster code from HW7\n",
    "\n",
    "#set up to remove stopwords\n",
    "new_stops = list(stopwords.words('english'))\n",
    "\n",
    "new_stops = new_stops + ['show', 'episode', 'it', 'get', ' it.', 'it\\'s', 'series', 'season', \n",
    "                         'can', 'no', 've', 'watch', 'watched', 'much', 'lot', 'thus', 'no', 'need', 'needed', 'should\\'ve',\n",
    "                        'it,', 'can.', 'can,']\n",
    "stop_words = new_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster code from HW7\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "from string import punctuation\n",
    "output = []\n",
    "counter = 1\n",
    "\n",
    "\n",
    "for i in reviews:\n",
    "    lst = []\n",
    "    if i not in stop_words:\n",
    "        lst.append(i)\n",
    "    span_nostop = ' '.join(lst)\n",
    "    output.append(str(span_nostop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster code from HW7\n",
    "\n",
    "output = []\n",
    "\n",
    "#Append reviews to a list for vectorizer\n",
    "\n",
    "for i in link_list:\n",
    "    lst = []\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    span = str(soup.find('div', attrs={'text show-more__control'}).text).strip().lower()\n",
    "    for i in span.split(' '):\n",
    "        if i not in stop_words:\n",
    "            lst.append(i)\n",
    "    span_nostop = ' '.join(lst)\n",
    "    \n",
    "    #span_nostop = ' '.join([i for i in span if i not in stop_words])\n",
    "    #tokenized = word_tokenize(span)\n",
    "    #tagged = nltk.pos_tag(tokenized)\n",
    "    #chunked = np.parse(tagged)\n",
    "    #for subtree in chunked.subtrees():\n",
    "       # if subtree.label() == 'NP':\n",
    "            #lst.append(subtree)\n",
    "    #span = [i for i in span if i not in string.punctuaion]\n",
    "    output.append(str(span_nostop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2547)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "#Perform tf-idf on reviews\n",
    "links_vect = vectorizer.fit_transform(output)\n",
    "#Look at shape\n",
    "links_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#Cluster\n",
    "kmeans = KMeans(n_clusters=6, random_state=11)\n",
    "clustered = kmeans.fit(links_vect)\n",
    "\n",
    "#Get cluster labels\n",
    "clusters = clustered.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, 4, 4, 1, 5, 4, 1, 4, 0, 2, 1, 3, 3, 1, 3, 4, 5, 4, 0, 0,\n",
       "       4, 3, 2, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 1, 1,\n",
       "       1, 1, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2,\n",
       "       3, 2, 0, 2, 0, 5, 0, 2, 2, 2, 3, 3, 3, 0, 3, 5, 3, 3, 3, 3, 0, 1,\n",
       "       5, 0, 3, 5, 3, 1, 3, 0, 0, 3, 2, 3])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_matrix2 = np.column_stack((sentiment, clusters))\n",
    "sent_matrix2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6       , 3.        ],\n",
       "       [0.15833333, 4.        ],\n",
       "       [0.6225    , 4.        ],\n",
       "       [0.40551146, 4.        ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check that it worked correclty\n",
    "sent_matrix2[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean, median, high, low for each cluster\n",
    "\n",
    "#Create nested dictionary to store results\n",
    "results_dict = {}\n",
    "for i in sent_matrix2[:,1]:\n",
    "    if i not in results_dict.keys():\n",
    "        results_dict[i]={}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4.0: {}, 3.0: {}, 1.0: {}, 5.0: {}, 0.0: {}, 2.0: {}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results_dict.keys():\n",
    "    tmp = []\n",
    "    for j in sent_matrix2:\n",
    "        if j[1]==i:\n",
    "            tmp.append(j[0])\n",
    "            results_dict[i]['mean']=np.mean(tmp)\n",
    "            results_dict[i]['median']=np.median(tmp)\n",
    "            results_dict[i]['max']=np.amax(tmp)\n",
    "            results_dict[i]['min']=np.amin(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4.0: {'mean': 0.3023495982755242,\n",
       "  'median': 0.2920634920634921,\n",
       "  'max': 0.6225,\n",
       "  'min': -0.275},\n",
       " 3.0: {'mean': 0.15000986900085223,\n",
       "  'median': 0.1267543859649123,\n",
       "  'max': 0.75,\n",
       "  'min': -0.36666666666666664},\n",
       " 1.0: {'mean': 0.1291113138994921,\n",
       "  'median': 0.12463765498976767,\n",
       "  'max': 0.37006802721088433,\n",
       "  'min': -0.2272727272727273},\n",
       " 5.0: {'mean': 0.040767873216994405,\n",
       "  'median': 0.00442279942279942,\n",
       "  'max': 0.38749999999999996,\n",
       "  'min': -0.177536231884058},\n",
       " 0.0: {'mean': 0.20188551483272887,\n",
       "  'median': 0.25,\n",
       "  'max': 0.5499999999999999,\n",
       "  'min': -0.15863636363636358},\n",
       " 2.0: {'mean': 0.09438201507639646,\n",
       "  'median': 0.10964285714285715,\n",
       "  'max': 0.35714285714285715,\n",
       "  'min': -0.37500000000000006}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean, median, max, and min polarity scores of each cluster was a little surprising to me at first.  The mean polarity scores for each cluster is positive (>.05) with the exception of cluster 5 (0.04 or 'neutral'). I had originally expected to see a mixture of positive and negative clusters, however, after tinking about it the outcome above makes sense.  The clusters were constructed based on the content of each review, and ended up clustering the different shows together instead of sentiment-based.  It makes sense that the polarity scores in each cluster would vary so greatly in terms of min and max. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    ">a. Take the chunks from homework 5, and in Python, run each chunk individually through your sentiment analyzer that you used in question 1. If the chunk registers a nonneutral sentiment, save it in a tabular format (the chunk, the sentiment score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NP Chunker from HW5\n",
    "\n",
    "output = {}\n",
    "counter = 1\n",
    "\n",
    "np_rule = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "np = nltk.RegexpParser(np_rule)\n",
    "\n",
    "for i in link_list:\n",
    "    lst = []\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    span = str(soup.find('div', attrs={'text show-more__control'}).text).strip()\n",
    "    tokenized = word_tokenize(span)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    chunked = np.parse(tagged)\n",
    "    for subtree in chunked.subtrees():\n",
    "        if subtree.label() == 'NP':\n",
    "            lst.append(subtree)\n",
    "    \n",
    "    output[\"Review: \" + str(counter)] = lst\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array of the chuncks\n",
    "chunk_list = []\n",
    "for k,v in output.items():\n",
    "    chunk_list.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list we can extract the words from\n",
    "chunks = []\n",
    "for i in chunk_list:\n",
    "    for j in i:\n",
    "        chunks.append(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get words from chucnks\n",
    "tmp = []\n",
    "for i in chunks:\n",
    "    for j in i:\n",
    "        tmp.append(j)\n",
    "\n",
    "#list of just the words\n",
    "words = tmp[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get polarity scores\n",
    "chunk_sent = []\n",
    "for i in words:\n",
    "    \n",
    "    #Blob it\n",
    "    blob = TextBlob(i)\n",
    "\n",
    "    #Sentiment Analysis - this funciton removes punctuation and tokenizes \n",
    "    sent = blob.sentiment\n",
    "\n",
    "    #add to sentiment list\n",
    "    chunk_sent.append(sent.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2744\n",
      "2744\n"
     ]
    }
   ],
   "source": [
    "#ensure the chunks and polarity scores are the same length\n",
    "print(len(chunk_sent))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sent_table = pd.DataFrame({'Chunk': chunks, 'Sentiment Score': chunk_sent})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(this, DT)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(a, DT)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(dinner, NN)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(the, DT)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(first, JJ)</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2739</td>\n",
       "      <td>(This, DT)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>(something, NN)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2741</td>\n",
       "      <td>(any, DT)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2742</td>\n",
       "      <td>(the, DT)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2743</td>\n",
       "      <td>(the, DT)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Chunk  Sentiment Score\n",
       "0          (this, DT)             0.00\n",
       "1             (a, DT)             0.00\n",
       "2        (dinner, NN)             0.00\n",
       "3           (the, DT)             0.00\n",
       "4         (first, JJ)             0.25\n",
       "...               ...              ...\n",
       "2739       (This, DT)             0.00\n",
       "2740  (something, NN)             0.00\n",
       "2741        (any, DT)             0.00\n",
       "2742        (the, DT)             0.00\n",
       "2743        (the, DT)             0.00\n",
       "\n",
       "[2744 rows x 2 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_sent_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    ">b.\tNow sort the table twice, once to show the highest negative-sentiment-scoring chunks at the top and again to show the highest positive-sentiment-scoring chunks at the top. Examine the upper portions of both sorted lists, to identify any trends, and explain what you see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort by highest positive scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>(excellent, JJ)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1097</td>\n",
       "      <td>(best, JJS)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>(awesome.., JJ)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>(Excellent, JJ)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2316</td>\n",
       "      <td>(priceless, NN)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1426</td>\n",
       "      <td>(brilliant, JJ)</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>(brilliant, NN)</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>(intelligent, NN)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>(great, JJ)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>(Great, JJ)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>(Great, NNP)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1138</td>\n",
       "      <td>(intelligent, JJ)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>(joy, NN)</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2274</td>\n",
       "      <td>(good, JJ)</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2578</td>\n",
       "      <td>(wise, NN)</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>(talented, JJ)</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>(Surprising, NNP)</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>(kind, NN)</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>(own, JJ)</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2381</td>\n",
       "      <td>(spectacular, JJ)</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>(nice, JJ)</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>(many, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>(more, JJR)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>(most, JJS)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2226</td>\n",
       "      <td>(humorous, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>(top, NN)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1626</td>\n",
       "      <td>(love, NN)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>(hilarious, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>(favorite, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1467</td>\n",
       "      <td>(creative, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>(convincing, NN)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>(refreshing, NN)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1644</td>\n",
       "      <td>(Hilarious, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1642</td>\n",
       "      <td>(memorable, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>(sexual, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>(top, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2452</td>\n",
       "      <td>(Many, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2397</td>\n",
       "      <td>(witty, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>(Interesting, NNP)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>(interesting, JJ)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Chunk  Sentiment Score\n",
       "85       (excellent, JJ)              1.0\n",
       "1097         (best, JJS)              1.0\n",
       "116      (awesome.., JJ)              1.0\n",
       "12       (Excellent, JJ)              1.0\n",
       "2316     (priceless, NN)              1.0\n",
       "1426     (brilliant, JJ)              0.9\n",
       "678      (brilliant, NN)              0.9\n",
       "252    (intelligent, NN)              0.8\n",
       "25           (great, JJ)              0.8\n",
       "24           (Great, JJ)              0.8\n",
       "615         (Great, NNP)              0.8\n",
       "1138   (intelligent, JJ)              0.8\n",
       "395            (joy, NN)              0.8\n",
       "2274          (good, JJ)              0.7\n",
       "2578          (wise, NN)              0.7\n",
       "628       (talented, JJ)              0.7\n",
       "14     (Surprising, NNP)              0.7\n",
       "316           (kind, NN)              0.6\n",
       "582            (own, JJ)              0.6\n",
       "2381   (spectacular, JJ)              0.6\n",
       "26            (nice, JJ)              0.6\n",
       "1670          (many, JJ)              0.5\n",
       "859          (more, JJR)              0.5\n",
       "2160         (most, JJS)              0.5\n",
       "2226      (humorous, JJ)              0.5\n",
       "1296           (top, NN)              0.5\n",
       "1626          (love, NN)              0.5\n",
       "551      (hilarious, JJ)              0.5\n",
       "877       (favorite, JJ)              0.5\n",
       "1467      (creative, JJ)              0.5\n",
       "565     (convincing, NN)              0.5\n",
       "844     (refreshing, NN)              0.5\n",
       "1644     (Hilarious, JJ)              0.5\n",
       "1642     (memorable, JJ)              0.5\n",
       "1250        (sexual, JJ)              0.5\n",
       "157            (top, JJ)              0.5\n",
       "2452          (Many, JJ)              0.5\n",
       "2397         (witty, JJ)              0.5\n",
       "251   (Interesting, NNP)              0.5\n",
       "21     (interesting, JJ)              0.5"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cst_top = chunk_sent_table.sort_values(by='Sentiment Score', ascending=False).drop_duplicates()\n",
    "\n",
    "cst_top.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort by lowest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2012</td>\n",
       "      <td>(Worst, NNP)</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>(outrageous, JJ)</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>(boring, JJ)</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>(idiots, NNS)</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2486</td>\n",
       "      <td>(annoying, JJ)</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>(violent, NN)</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>(hate, NN)</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>(crap, NN)</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2669</td>\n",
       "      <td>(stupid, JJ)</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>(crude, NN)</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>(painful, JJ)</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1802</td>\n",
       "      <td>(vulgar, JJ)</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1624</td>\n",
       "      <td>(crude, JJ)</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2604</td>\n",
       "      <td>(bad, JJ)</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2558</td>\n",
       "      <td>(BAD, NNP)</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>(Bad, NNP)</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>(Awkward, NNP)</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(crazy, JJ)</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1811</td>\n",
       "      <td>(stupidity, NN)</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2518</td>\n",
       "      <td>(deadpan, NN)</td>\n",
       "      <td>-0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2649</td>\n",
       "      <td>(casual, JJ)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1849</td>\n",
       "      <td>(insecure, JJ)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2053</td>\n",
       "      <td>(random, NN)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2027</td>\n",
       "      <td>(gratuitous, JJ)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>(random, JJ)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2028</td>\n",
       "      <td>(fake, NN)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1373</td>\n",
       "      <td>(illegal, JJ)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>(mere, JJ)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>(silly, JJ)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1063</td>\n",
       "      <td>(Sorry, NNP)</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>(shrieky, JJ)</td>\n",
       "      <td>-0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>(Game, NNP)</td>\n",
       "      <td>-0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>(shaky, NN)</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>(subtle, JJ)</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>(slow, JJ)</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2672</td>\n",
       "      <td>(forced, JJ)</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1913</td>\n",
       "      <td>(least, JJS)</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2031</td>\n",
       "      <td>(exploitative, JJ)</td>\n",
       "      <td>-0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>(usual, JJ)</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2032</td>\n",
       "      <td>(excessive, JJ)</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Chunk  Sentiment Score\n",
       "2012        (Worst, NNP)        -1.000000\n",
       "585     (outrageous, JJ)        -1.000000\n",
       "214         (boring, JJ)        -1.000000\n",
       "926        (idiots, NNS)        -0.800000\n",
       "2486      (annoying, JJ)        -0.800000\n",
       "15         (violent, NN)        -0.800000\n",
       "340           (hate, NN)        -0.800000\n",
       "992           (crap, NN)        -0.800000\n",
       "2669        (stupid, JJ)        -0.800000\n",
       "1725         (crude, NN)        -0.700000\n",
       "879        (painful, JJ)        -0.700000\n",
       "1802        (vulgar, JJ)        -0.700000\n",
       "1624         (crude, JJ)        -0.700000\n",
       "2604           (bad, JJ)        -0.700000\n",
       "2558          (BAD, NNP)        -0.700000\n",
       "76            (Bad, NNP)        -0.700000\n",
       "878       (Awkward, NNP)        -0.600000\n",
       "6            (crazy, JJ)        -0.600000\n",
       "1811     (stupidity, NN)        -0.600000\n",
       "2518       (deadpan, NN)        -0.550000\n",
       "2649        (casual, JJ)        -0.500000\n",
       "1849      (insecure, JJ)        -0.500000\n",
       "2053        (random, NN)        -0.500000\n",
       "2027    (gratuitous, JJ)        -0.500000\n",
       "763         (random, JJ)        -0.500000\n",
       "2028          (fake, NN)        -0.500000\n",
       "1373       (illegal, JJ)        -0.500000\n",
       "434           (mere, JJ)        -0.500000\n",
       "629          (silly, JJ)        -0.500000\n",
       "1063        (Sorry, NNP)        -0.500000\n",
       "2060       (shrieky, JJ)        -0.400000\n",
       "1218         (Game, NNP)        -0.400000\n",
       "317          (shaky, NN)        -0.333333\n",
       "801         (subtle, JJ)        -0.333333\n",
       "1003          (slow, JJ)        -0.300000\n",
       "2672        (forced, JJ)        -0.300000\n",
       "1913        (least, JJS)        -0.300000\n",
       "2031  (exploitative, JJ)        -0.300000\n",
       "668          (usual, JJ)        -0.250000\n",
       "2032     (excessive, JJ)        -0.250000"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cst_low = chunk_sent_table.sort_values(by='Sentiment Score', ascending=True).drop_duplicates()\n",
    "cst_low.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polarity scores of the words seems to make sense for the most part.  A couple of interesting observations are some of the words that have high positive scores.  For exapmle, 'own' has a polarity score of 0.6.  That seems rather odd to me as it seems to be a little ambiguous for such a high score.  On the negative side, it seems odd that 'hate' only has a polarit score of -0.8. I would consider 'hate' to be perhaps one of the top words used to convey a negative sentiment.  As the scores get into the -.4's and -.3's it seems like the words become more contextual, which makes sense that they are closer to zero.  For example, 'subtle' is -0.33. I could see how 'subtle' could be used in a positive or negative context.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
