{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from urllib.request import urlopen\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#School Reading by Grades: Sixth Year\n",
    "\n",
    "tt1 = \"School Readings by Grades: Sixth Year\"\n",
    "url = \"http://www.gutenberg.org/files/36864/36864-0.txt\"\n",
    "response = urlopen(url)\n",
    "txt1 = response.read().decode('utf8')\n",
    "type(txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#School Reading By Grades: Fifth Year\n",
    "\n",
    "tt2 = \"School Reading By Grades: Fifth Year\"\n",
    "url = \"http://www.gutenberg.org/files/51000/51000-0.txt\"\n",
    "response = urlopen(url)\n",
    "txt2 = response.read().decode('utf8')\n",
    "type(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Little Dramas for Primary Grades\n",
    "\n",
    "tt3 = \"Little Dramas for Primary Grades\"\n",
    "url = \"http://www.gutenberg.org/files/53245/53245-0.txt\"\n",
    "response = urlopen(url)\n",
    "txt3 = response.read().decode('utf8')\n",
    "type(txt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Morning in the West\n",
    "\n",
    "tt4 = \"Morning in the West\"\n",
    "url = \"http://www.gutenberg.org/cache/epub/63153/pg63153.txt\"\n",
    "response = urlopen(url)\n",
    "txt4 = response.read().decode('utf8')\n",
    "type(txt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Men Who Have Made the Empire\n",
    "\n",
    "tt5 = \"Men Who Have Made the Empire\"\n",
    "url = \"https://www.gutenberg.org/files/63148/63148-0.txt\"\n",
    "response = urlopen(url)\n",
    "txt5 = response.read().decode('utf8')\n",
    "type(txt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School Readings by Grades: Sixth Year',\n",
       " 'School Reading By Grades: Fifth Year',\n",
       " 'Little Dramas for Primary Grades',\n",
       " 'Morning in the West',\n",
       " 'Men Who Have Made the Empire']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save our titles for later\n",
    "titles = [tt1, tt2, tt3, tt4, tt5]\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411686"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Double check to make sure the texts were imported correclty\n",
    "len(txt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Little Dramas for Primary Grades, by \\r\\nAda '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text sample to ensure we imported correctly\n",
    "txt3[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize each text so we can find the vocab size of each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all texts into one list so we can iterate through\n",
    "text_list = [txt1, txt2, txt3, txt4, txt5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with the tokenized texts\n",
    "tok_textlist = []\n",
    "for i in text_list:\n",
    "    tokens = word_tokenize(i)\n",
    "    i = nltk.Text(tokens)\n",
    "    tok_textlist.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "72533\n",
      "<class 'nltk.text.Text'>\n",
      "55249\n",
      "<class 'nltk.text.Text'>\n",
      "22137\n",
      "<class 'nltk.text.Text'>\n",
      "10355\n",
      "<class 'nltk.text.Text'>\n",
      "79868\n"
     ]
    }
   ],
   "source": [
    "#Ensure we have the correct data type\n",
    "for i in tok_textlist:\n",
    "    print(type(i))\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1.)\n",
    ">1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Size Funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to score vocab size\n",
    "#Note: Function takes in a list of texts\n",
    "def vocab_size(x):\n",
    "    vsize = []\n",
    "    normed = []\n",
    "    \n",
    "    for i in x:\n",
    "        vsize.append(len(set(i)))\n",
    "        \n",
    "    for j in vsize:\n",
    "        normed.append((j - min(vsize)) / (max(vsize) - min(vsize)))\n",
    "    return (normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.8460052435490548, 0.056713122671450254, 0.0, 0.915275286325376]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size(tok_textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method I used to normalize the data was to do the actual formula of:\n",
    "> (Score  - Min_Score) / (Max_Score – Min_Score)\n",
    "\n",
    "This will bring the scores to a value between 0 and 1.  This will allow us to use other metrics without having to be concerned about the varying scales of the metrics and actually compare apples to apples.   If we did not use a normalization technique, we wouldn't be able to relaly use vocab size, long word score, and lexical diversity in the same sentence since the scale for all three are so different. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.)\n",
    ">2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Word Score Function from Chapter 1 Section 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_words(x):\n",
    "    long_list = []\n",
    "    nlong_list = []\n",
    "    temp_cnt = 0\n",
    "    for i in x:\n",
    "        for j in i: \n",
    "            if len(j)>=15:\n",
    "                temp_cnt += 1\n",
    "        long_list.append(temp_cnt)\n",
    "        temp_cnt = 0\n",
    "    \n",
    "    #Normalization of scores     \n",
    "    for j in long_list:\n",
    "        nlong_list.append((j - min(long_list)) / (max(long_list) - min(long_list)))\n",
    "    return nlong_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12844036697247707, 0.009174311926605505, 0.22018348623853212, 0.0, 1.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_words(tok_textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we normalize the scores so they can be compatible with the scores from question 1.  If we were not to do this, the scales of the two would be drastically different, and combining the two would not be comparing apples to apples.  For example, long-word vocab size of a text is somewhere between 10 and 60.  The vocab size score from question 1 is at least one, more so two levels of magnitude larger.  Once we normalize them, they are on the same playing field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.)\n",
    ">3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lexical Diversity Funciton from HW 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_diversity(x):\n",
    "    lex_list = []\n",
    "    \n",
    "    for i in x:\n",
    "        lex_list.append(len(set(i)) / len(i))\n",
    "    return lex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13745467580273807,\n",
       " 0.1602562942315698,\n",
       " 0.1415729321949677,\n",
       " 0.26296475132786096,\n",
       " 0.11714328642259729]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_diversity(tok_textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Difficulty Function Combining All Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difficulty(x):\n",
    "    #run our three functions against text list\n",
    "    vs = np.array(vocab_size(x))\n",
    "    lw = np.array(long_words(x))\n",
    "    ld = np.array(lex_diversity(x))\n",
    "    \n",
    "    #convert into matrix where the text is the column value\n",
    "    score_matrix = np.concatenate(np.array([vs, lw, ld]), axis=0)\n",
    "    \n",
    "    #reshape the matrix and sum the 'columns' \n",
    "    score_matrix.shape = (3, len(x))\n",
    "    final_scores = score_matrix.sum(axis=0)\n",
    "\n",
    "    #combine final_scores and titles into sorted dictionary to easily identify the most and least difficult\n",
    "    tmp = {j:i for i,j in sorted(zip(final_scores, titles), reverse=True)}\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Men Who Have Made the Empire': 2.0324185727479733,\n",
       " 'School Readings by Grades: Sixth Year': 1.2658950427752154,\n",
       " 'School Reading By Grades: Fifth Year': 1.01543584970723,\n",
       " 'Little Dramas for Primary Grades': 0.41846954110495005,\n",
       " 'Morning in the West': 0.26296475132786096}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_difficulty(tok_textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken more than one factor into account, the text difficulty scores of the graded texts seem to align with my expectations—the texts would get more difficult as the grades increased.  The first three texts I used in this assignment were the three texts from homework one and decreased in grade from 1-3 (sixth, fifth, primary).  The corresponding text difficulty scores were 1.266, 1.012, and 0.418.  This seems much more reasonable than using lexical diversity or vocabulary size of a text alone.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
